{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5497796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from helper import get_nan_count, get_missing_val_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3c427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('../resources/data/HTx_ind_treat_res_new_data_update_without_dates_08032024.csv', sep = ';',decimal = ',', encoding = 'utf-8', engine ='python')\n",
    "df_date = pd.read_csv('../resources/data/HTx_ind_treat_res_new_data_update_only_dates_08032024.csv', sep = ';',decimal = ',', encoding = 'utf-8', engine ='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36def5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read common variables from a YAML file\n",
    "with open('../common_variables.yaml', 'r') as file:\n",
    "    common_data = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5474ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13882, 238)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename postinumero with id\n",
    "df = df_new.rename(columns={'potilasnumero': 'id'})\n",
    "df_date = df_date.rename(columns={'potilasnumero': 'id'})\n",
    "\n",
    "columns_to_add = common_data['columns_to_add']\n",
    "\n",
    "# Add selected columns from dfdate to df\n",
    "for col in columns_to_add:\n",
    "    df[col] = df_date[col]\n",
    "\n",
    "is_train_with_all = False\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a571b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# response_variable = 'hba1c_12m'\n",
    "response_variable_list = common_data['response_variable_list']\n",
    "\n",
    "correlated_variables = common_data['correlated_variables']\n",
    "\n",
    "variables = df.columns\n",
    "thresh = common_data['thresh']\n",
    "keep = []\n",
    "rem = []\n",
    "\n",
    "def preprocess(df, test_size):\n",
    "    print(\"original shape: \", np.shape(df))\n",
    "    # find duplicates and keep first record only\n",
    "#     df = df.sort_values(\"id\").drop_duplicates(subset = ['id'], keep='first')\n",
    "#     print('Shape of data after removing duplicate ids:', np.shape(df))\n",
    "    \n",
    "    # remove all the records with drug class is not 2,3,or 4 \n",
    "#     2=GLP-1 analogues (A10BJ)\n",
    "#     3=DPP-4 inhibitors (A10BH)\n",
    "#     4=SGLT2 inhibitors (A10BK)\n",
    "    if(not is_train_with_all):\n",
    "        df = df[\n",
    "                (df['drug_class'] == 3) |\n",
    "                (df['drug_class'] == 4) ]\n",
    "\n",
    "    # replace ' ' as NaN\n",
    "    df = df.replace(' ', np.NaN)\n",
    "    print('Shape of data after removing other drug types:', np.shape(df))\n",
    "    \n",
    "    # filter by bmi\n",
    "    df['bmi'] = df['bmi'].astype(float)\n",
    "    df['sp'] = df['sp'].astype(int)\n",
    "    df['ika'] = df['ika'].astype(float)\n",
    "    df['smoking'] = df['smoking'].astype(float)\n",
    "\n",
    "    # remove rows with missing 'response variable'\n",
    "    get_nan_count(df)\n",
    "#     df = df.dropna(how='any', subset = response_variable_list)\n",
    "    print('Shape of data after excluding missing response:', np.shape(df))\n",
    "    \n",
    "    #delete columns with more than threshold NaN\n",
    "    # get missing values < threshold feature name list\n",
    "    missing_per = get_missing_val_percentage(df)\n",
    "    \n",
    "    for i in range(df.columns.shape[0]):\n",
    "        if missing_per[i] <= 42: #setting the threshold as 40%\n",
    "            keep.append(variables[i])\n",
    "        else :\n",
    "            rem.append(variables[i])\n",
    "    \n",
    "    columns_to_remove = ['hba1c_prev_1y', 'date_hdl_12m', 'date_bmi_12m','date_ldl_12m',\n",
    "                        'hba1c_12m', 'ldl_12m', 'hdl_12m', 'bmi_12m']\n",
    "    \n",
    "    for col in columns_to_remove:\n",
    "        if col in rem:\n",
    "            rem.remove(col)\n",
    "    \n",
    "    df = df.drop([x for x in rem if x in df.columns], axis=1)\n",
    "    print('Shape of data after removing cols with less than %.2f percent values missing:' % (thresh), np.shape(df))\n",
    "        \n",
    "    #df = df.drop('id', axis=1)\n",
    "    \n",
    "    #     remove correlated features \n",
    "    df = df.drop([x for x in correlated_variables if x in df.columns], axis=1)\n",
    "    print('Shape of data after removing correlated features:', np.shape(df))\n",
    "    \n",
    "    # convert categorical to numeric\n",
    "    cat_cols = []\n",
    "    for i in cat_cols:\n",
    "        labelencoder = LabelEncoder()\n",
    "        df[i] = labelencoder.fit_transform(df[i])\n",
    "        \n",
    "    ## insert days\n",
    "    \n",
    "    date_cols = ['date_hba_bl_6m','date_ldl_bl','date_bmi_bl','date_hdl_bl',\n",
    "                 'date_12m', 'date_n1',\n",
    "                 'date_ldl_12m',\n",
    "                 'date_bmi_12m',\n",
    "                 'date_hdl_12m']\n",
    "    \n",
    "    #convert dates into datetime format\n",
    "    df[date_cols] = df[date_cols].apply(pd.to_datetime, errors='coerce', format='%m/%d/%Y')#, exact= False)\n",
    "    days_to_response_hba1c = df['date_12m'] - df['date_hba_bl_6m']\n",
    "    days_to_response_bmi = df['date_bmi_12m'] - df['date_bmi_bl']\n",
    "    days_to_response_hdl = df['date_hdl_12m'] - df['date_hdl_bl']\n",
    "    days_to_response_ldl = df['date_ldl_12m'] - df['date_ldl_bl']\n",
    "    \n",
    "    df.loc[:,'days_hba1c'] = [x.days for x in days_to_response_hba1c]\n",
    "    df.loc[:,'days_bmi'] = [x.days for x in days_to_response_bmi]\n",
    "    df.loc[:,'days_hdl'] = [x.days for x in days_to_response_hdl]\n",
    "    df.loc[:,'days_ldl'] = [x.days for x in days_to_response_ldl]\n",
    "    \n",
    "    print('Shape of full data with change + days', np.shape(df))\n",
    "\n",
    "    #convert other \"object\" columns to numeric \n",
    "    convert = df.select_dtypes('object').columns\n",
    "    df.loc[:, convert] = df[convert].apply(pd.to_numeric, downcast='float', errors='coerce')\n",
    "    \n",
    "    # select time interval\n",
    "    start = 21\n",
    "    end = 365 #426\n",
    "#     df = df[(df['days_hba1c'] >= start)]\n",
    "#     df = df[(df['days_bmi'] >= start)]\n",
    "#     df = df[(df['days_hdl'] >= start)]\n",
    "#     df = df[(df['days_ldl'] >= start)]\n",
    "    print('Shape of full data after selecting date range dates > 21 days', np.shape(df))\n",
    "    \n",
    "#     df = df.drop(date_cols, axis=1)\n",
    "#     df = df.drop(['days_hba1c','days_bmi','days_hdl','days_ldl'], axis=1)\n",
    "\n",
    "#     df = df.drop(['days_hba1c', 'days_bmi', 'days_hdl','days_ldl'], axis = 1)\n",
    "    # remove outliers\n",
    "    \n",
    "#     df = df.astype(float)\n",
    "    \n",
    "    \n",
    "    # filter by hba1c baseline levels and egfr levels\n",
    "    df['hba1c_bl_6m'] = df['hba1c_bl_6m'].apply(pd.to_numeric, downcast='float', errors='coerce')\n",
    "    df['eGFR'] = df['eGFR'].apply(pd.to_numeric, downcast='float', errors='coerce')\n",
    "    \n",
    "    criteria = (df['hba1c_bl_6m'] < 53) | (df['hba1c_bl_6m'] > 119) | (df['eGFR'] < 45)\n",
    "    print('baseline hba1c min ', df['hba1c_bl_6m'].min(), ' and max ', df['hba1c_bl_6m'].max())\n",
    "    \n",
    "    print(f'\\n Number of samples after the filteration of baseline hba1c and eGFR: {criteria.sum()}')\n",
    "\n",
    "    print('df shape before the filteration ', df.shape)\n",
    "\n",
    "    # Invert the criteria to keep only the rows that do not meet any of the criteria\n",
    "    df = df[~criteria]\n",
    "    print('df shape after the filteration ', df.shape)\n",
    "    \n",
    "    \n",
    "    # split data\n",
    "    random.seed(42)\n",
    "    # Save original data set\n",
    "    original = df\n",
    "    Y = df[response_variable_list]\n",
    "    X = df.drop(response_variable_list, axis=1)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=123)\n",
    "    # save preprocessed df to csv\n",
    "    result_df = pd.concat([X_train, Y_train], axis=1)\n",
    "    test_df = pd.concat([X_test, Y_test], axis = 1)\n",
    "\n",
    "    result_df.to_csv('data/X_train.csv', index=True)\n",
    "    test_df.to_csv('data/X_test.csv', index=True)\n",
    "    \n",
    "    \n",
    "#     # data imputation\n",
    "#     original_X_train = X_train\n",
    "#     original_X_test = X_test\n",
    "#     random.seed(42)\n",
    "#     imputer = SimpleImputer(missing_values=np.nan, strategy = \"most_frequent\")\n",
    "#     # imputeX = KNNImputer(missing_values=np.nan, n_neighbors = 3, weights='distance')\n",
    "#     # imputeX = IterativeImputer(max_iter=5, random_state=0)\n",
    "#     X_train = imputer.fit_transform(X_train)\n",
    "#     X_test = imputer.transform(X_test)\n",
    "    \n",
    "#     X_train = pd.DataFrame(X_train, columns = original_X_train.columns, index=original_X_train.index)\n",
    "#     X_test = pd.DataFrame(X_test, columns = original_X_train.columns, index=original_X_test.index)\n",
    "    \n",
    "#     #     columns_to_skip_normalization = ['drug_class']\n",
    "#     columns_to_skip_normalization = []\n",
    "#     # List of columns to normalize\n",
    "#     columns_to_normalize = [col for col in X_train.columns if col not in columns_to_skip_normalization]\n",
    "\n",
    "#     # scale data \n",
    "# #     scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "#     select = {}\n",
    "#     X_train[columns_to_normalize] = scaler.fit_transform(X_train[columns_to_normalize])\n",
    "#     X_test[columns_to_normalize] = scaler.transform(X_test[columns_to_normalize])\n",
    "    \n",
    "    # random oversampling \n",
    "#     combined_df = pd.concat([X_train, Y_train], axis=1)\n",
    "#     X_oversamp = combined_df.drop(['drug_class'], axis = 1)\n",
    "#     Y_oversamp = combined_df['drug_class']\n",
    "# #     ros = RandomOverSampler(random_state=0)\n",
    "#     smote = SMOTE()\n",
    "#     X_resampled, y_resampled = smote.fit_resample(X_oversamp, Y_oversamp)\n",
    "#     print(sorted(Counter(Y_oversamp).items()))\n",
    "#     print(sorted(Counter(y_resampled).items()))\n",
    "#     combined = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "#     X_train = combined_df.drop([response_variable], axis = 1)\n",
    "#     Y_train = combined_df[response_variable]\n",
    "    \n",
    "#     X_train[columns_to_normalize] = scaler.fit_transform(X_train[columns_to_normalize])\n",
    "#     X_test[columns_to_normalize] = scaler.transform(X_test[columns_to_normalize])\n",
    "    \n",
    "    return df, X_train, X_test, Y_train, Y_test, X, Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13838075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape:  (13882, 238)\n",
      "Shape of data after removing other drug types: (5480, 238)\n",
      "\n",
      " NaN counts in resonse variables:\n",
      "             Feature  NaN Count\n",
      "hba1c_12m  hba1c_12m        697\n",
      "ldl_12m      ldl_12m       2276\n",
      "hdl_12m      hdl_12m       3982\n",
      "bmi_12m      bmi_12m       2003\n",
      "\n",
      "Shape of data after excluding missing response: (5480, 238)\n",
      "Shape of data after removing cols with less than 40.00 percent values missing: (5480, 184)\n",
      "Shape of data after removing correlated features: (5480, 122)\n",
      "Shape of full data with change + days (5480, 126)\n",
      "Shape of full data after selecting date range dates > 21 days (5480, 126)\n",
      "baseline hba1c min  23.0  and max  164.0\n",
      "\n",
      " Number of samples after the filteration of baseline hba1c and eGFR: 1933\n",
      "df shape before the filteration  (5480, 126)\n",
      "df shape after the filteration  (3547, 126)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r6/l8_rty2n33d80z3fh81nvndm0000gn/T/ipykernel_92368/3278289126.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if missing_per[i] <= 42: #setting the threshold as 40%\n"
     ]
    }
   ],
   "source": [
    "df, X_train, X_test, Y_train, Y_test, X, Y = preprocess(df, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96dfb161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1491"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ldl_12m'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c58de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3        1.5\n",
       "8        NaN\n",
       "10       3.6\n",
       "19       NaN\n",
       "30       1.8\n",
       "        ... \n",
       "13858    2.2\n",
       "13862    NaN\n",
       "13863    NaN\n",
       "13878    NaN\n",
       "13880    3.9\n",
       "Name: ldl_12m, Length: 3547, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ldl_12m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9fefa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
